{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10 - Web Mining\n",
    "## Advait Deochakke\n",
    "###     20BCE1143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 6 nodes and 16 edges\n"
     ]
    }
   ],
   "source": [
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add the nodes\n",
    "G.add_nodes_from(['A', 'B', 'C', 'D', 'E', 'F'])\n",
    "\n",
    "# Add the edges\n",
    "G.add_edges_from([('A', 'B'), ('A', 'D'), ('A', 'E'), ('A', 'F'),\n",
    "                  ('B', 'A'), ('B', 'C'), ('B', 'D'),\n",
    "                  ('C', 'B'), ('C', 'D'),\n",
    "                  ('D', 'A'), ('D', 'B'), ('D', 'C'),\n",
    "                  ('E', 'A'), ('E', 'F'),\n",
    "                  ('F', 'A'), ('F', 'E')])\n",
    "# it takes you about two hours to realize that working with a complete weighted, directed graph with\n",
    "# base weight as zero to show no edge and weight as 1 to show edge\n",
    "# is so much easier to work with than whatever i was tyring to do above\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.24432008217628517,\n",
       " 'B': 0.18404146267950455,\n",
       " 'C': 0.12709167694914336,\n",
       " 'D': 0.18404146267950455,\n",
       " 'E': 0.1302526577577811,\n",
       " 'F': 0.1302526577577811}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.pagerank(G, alpha=0.9, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pagerank(G, alpha=0.85, max_iter=100, tol=1e-4, weighted=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function calculates the weighted page rank scores for a given graph G using the Weighted Page Rank algorithm as described by Wenpu Xing et al.\n",
    "    \n",
    "    :param G: The input graph\n",
    "    :type G: networkx.DiGraph\n",
    "    :param alpha: The damping factor (default value is 0.85)\n",
    "    :type alpha: float\n",
    "    :param max_iter: The maximum number of iterations to perform (default value is 100)\n",
    "    :type max_iter: int\n",
    "    :param tol: The tolerance used to check for convergence (default value is 1e-4)\n",
    "    :type tol: float\n",
    "    :return: A dictionary containing the page rank scores for each node in the graph\n",
    "    :rtype: dict\n",
    "    \n",
    "    :param weighted: an input to say if we use the WPR or normal version\n",
    "    default false\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(G.nodes)\n",
    "    node_list = list(G.nodes)\n",
    "\n",
    "    # Initializing the page rank scores\n",
    "    pageranks = {node: 1/n for node in node_list}\n",
    "    \n",
    "    # WPR algorithm\n",
    "    if weighted:\n",
    "        # print(\"entered weighted\")\n",
    "        for i in range(max_iter):\n",
    "            # print(\"iteration\", i)\n",
    "            new_pageranks = {node: 0 for node in node_list}\n",
    "            \n",
    "            for j in range(n):\n",
    "                sum_pr = 0\n",
    "                \n",
    "                reference_set = list(G.successors(node_list[j]))\n",
    "                sum_inlinks = sum(G.in_degree(node) for node in reference_set)\n",
    "                sum_outlinks = sum(G.out_degree(node) for node in reference_set)\n",
    "                # print(\"sum inlinks is\", sum_inlinks)\n",
    "                # print(\"current node is\", list(G.nodes)[j])\n",
    "                for node in reference_set:\n",
    "                    sum_pr += pageranks[node]*(G.in_degree(node)/sum_inlinks)*(G.out_degree(node)/sum_outlinks) \n",
    "                    # PR(u) * Win(v, u) * Wout(v, u)\n",
    "                    \n",
    "                new_pageranks[node_list[j]] = (1 - alpha) + alpha * sum_pr \n",
    "            \n",
    "            # Checking for convergence - boilerplate\n",
    "            converged = True\n",
    "            for node in node_list:\n",
    "                if abs(new_pageranks[node] - pageranks[node]) > tol:\n",
    "                    converged = False\n",
    "                    # print(\"the abs diff was greater than tol for \", node)\n",
    "                    break\n",
    "            if converged:\n",
    "                return pageranks\n",
    "            \n",
    "            pageranks = new_pageranks\n",
    "    \n",
    "    else:\n",
    "        # print(\"entered normal\")\n",
    "        for i in range(max_iter):\n",
    "            # print(\"iteration\", i)\n",
    "            new_pageranks = {node: 0 for node in node_list}\n",
    "            \n",
    "            for j in range(n):\n",
    "                sum_pr = 0\n",
    "                \n",
    "                reference_set = list(G.predecessors(node_list[j]))\n",
    "                for node in reference_set:\n",
    "                    sum_pr += pageranks[node]/(G.out_degree(node))\n",
    "                    \n",
    "                new_pageranks[node_list[j]] = (1 - alpha) + alpha * sum_pr \n",
    "            \n",
    "            # Checking for convergence - boilerplate\n",
    "            converged = True\n",
    "            for node in node_list:\n",
    "                if abs(new_pageranks[node] - pageranks[node]) > tol:\n",
    "                    converged = False\n",
    "                    # print(\"the abs diff was greater than tol for \", node)\n",
    "                    break\n",
    "            if converged:\n",
    "                return pageranks\n",
    "            \n",
    "            pageranks = new_pageranks\n",
    "    \n",
    "    return pageranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted page rank :\n",
      "{'A': 0.1996139589685682, 'B': 0.21379225462935914, 'C': 0.24082939716707513, 'D': 0.21379225462935914, 'E': 0.2488770935071386, 'F': 0.2488770935071386}\n",
      "normal page rank :\n",
      "{'A': 0.2410755932364277, 'B': 0.18249893766024683, 'C': 0.1284085396216346, 'D': 0.18249893766024686, 'E': 0.13255832784388386, 'F': 0.13255832784388386}\n"
     ]
    }
   ],
   "source": [
    "print(\"weighted page rank :\")\n",
    "wpr = calc_pagerank(G, weighted=True)\n",
    "print(wpr)\n",
    "print(\"normal page rank :\")\n",
    "npr = calc_pagerank(G, weighted=False)\n",
    "npr = {key: value/len(list(G.nodes)) for key, value in npr.items()}\n",
    "print(npr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page rank with networkx inbuilt :\n",
      "{'A': 0.24117597909181454, 'B': 0.18257382216198692, 'C': 0.12845786661086428, 'D': 0.1825738221619869, 'E': 0.13260925498667359, 'F': 0.13260925498667359}\n"
     ]
    }
   ],
   "source": [
    "print(\"page rank with networkx inbuilt :\")\n",
    "nxpr = nx.pagerank(G)\n",
    "print(nxpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
